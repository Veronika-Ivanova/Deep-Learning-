{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","version":"3.7.7","file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python"},"notebookId":"75a8d2fd-b586-4c7c-83c2-d3027dcfd79a"},"cells":[{"cell_type":"markdown","source":"# CV – object detection\n\nВ этой тетрадке мы рассмотрим задачу детекции объектов на примере датасета [Stanford Drone Dataset](https://cvgl.stanford.edu/projects/uav_data/)\n\n**Предполагаем, что ноутбук запущен внутри Yandex DataSphere**","metadata":{"id":"cIu1b5Xo4Bdd","cellId":"38lqsetzmcbfb8lk82ubeg"}},{"cell_type":"code","source":"from pathlib import Path\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport cv2\nimport os\nimport torch\nimport torch.nn as nn\n# import torch.nn.functional as F\nfrom torch import optim\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet34\n# from torch.utils.tensorboard import SummaryWriter\n\nfrom collections import Counter, defaultdict\nimport random","metadata":{"id":"TyTa9uFb4Bdl","cellId":"212z2memf5cmplpkb5eb0s","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%pip install seaborn\nimport seaborn as sns","metadata":{"cellId":"gv0n6wjclcfwq0iwohw0w","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: seaborn in /home/jupyter/.local/lib/python3.7/site-packages (0.11.1)\nRequirement already satisfied: numpy>=1.15 in /kernel/lib/python3.7/site-packages (from seaborn) (1.19.4)\nRequirement already satisfied: matplotlib>=2.2 in /kernel/lib/python3.7/site-packages (from seaborn) (3.3.3)\nRequirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\nRequirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (0.25.3)\nRequirement already satisfied: python-dateutil>=2.1 in /kernel/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\nRequirement already satisfied: cycler>=0.10 in /kernel/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /kernel/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /kernel/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (8.2.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /kernel/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\nRequirement already satisfied: six in /kernel/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\nRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2021.1)\n"}],"execution_count":2},{"cell_type":"code","source":"%pip install albumentations\nimport albumentations as A\nimport albumentations.pytorch.transforms as APT","metadata":{"cellId":"9f03qiejkd80q1cwyi2amcq","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: albumentations in /home/jupyter/.local/lib/python3.7/site-packages (0.5.2)\nRequirement already satisfied: scikit-image>=0.16.1 in /home/jupyter/.local/lib/python3.7/site-packages (from albumentations) (0.18.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (5.3.1)\nRequirement already satisfied: imgaug>=0.4.0 in /home/jupyter/.local/lib/python3.7/site-packages (from albumentations) (0.4.0)\nRequirement already satisfied: numpy>=1.11.1 in /kernel/lib/python3.7/site-packages (from albumentations) (1.19.4)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /home/jupyter/.local/lib/python3.7/site-packages (from albumentations) (4.5.1.48)\nRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\nRequirement already satisfied: Shapely in /home/jupyter/.local/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\nRequirement already satisfied: matplotlib in /kernel/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (3.3.3)\nRequirement already satisfied: imageio in /home/jupyter/.local/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (2.9.0)\nRequirement already satisfied: six in /kernel/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\nRequirement already satisfied: Pillow in /kernel/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (8.2.0)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations) (4.5.1.48)\nRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4)\nRequirement already satisfied: PyWavelets>=1.1.1 in /home/jupyter/.local/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\nRequirement already satisfied: tifffile>=2019.7.26 in /home/jupyter/.local/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.4.8)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /kernel/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /kernel/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /kernel/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.1 in /kernel/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\nRequirement already satisfied: decorator>=4.3.0 in /kernel/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (5.0.7)\n"}],"execution_count":3},{"cell_type":"markdown","source":"## Data\n\nStanford drone dataset – это датасет из нескольких видео, записанных с дрона в восьми местах (`SCENE_NAME`).\nКаждое видео покадрово размечено шестью типами объектов (`label`). (Подробное описание датасета лежит в файле `initial_README`)  \nВ текущей задаче мы будем использовать не исходные видео, а фреймы из них.\n\nСтруктура директории с датасетом:\n- ./stanford-drone-dataset-frames/\n    - {train, val}/\n        - annotations/\n            - {`SCENE_NAME`}/\n                - video{`VIDEO_ID`}/\n                    - annotations.csv\n    - {train, val}/\n        - frames/\n            - {`SCENE_NAME`}/\n                - video{`VIDEO_ID`}/\n                    - frame_{`FRAME_IDX`}.jpg\n\nДля каждого файла с аннотацией есть аналогичный путь к директории, в которой находятся фреймы из видео `frame_{FRAME_IDX}.jpg`.  \nНапример, для фреймов, лежащих в директории `./stanford-drone-dataset-frames/train/frames/quad/video0/`, соответствующий файл с разметкой лежит по пути: `./stanford-drone-dataset-frames/train/annotations/quad/video0/annotations.csv`.\n\nКаждая строка в файлах `annotations.csv` содержит аннотацию одного объекта. Каждый CSV файл содержит 11 колонок с хедером.\n\nНаиболее интересные нам колонки:\n- xmin – верхняя левая X-координата бокса объекта (bounding box).\n- ymin – верхняя левая Y-координата бокса объекта (bounding box).\n- xmax – нижняя правая X-координата бокса объекта (bounding box).\n- ymax – нижняя правая Y-координата бокса объекта (bounding box).\n- frame_idx – индекс фрейма, который соответсвует текущей строке аннотации.\n\nСэмпл из одной сцены можно скачать с [Google Drive](https://drive.google.com/file/d/18XeE0kHWqpLyBfZFbAbUTZqbygETTCLC/view?usp=sharing).","metadata":{"id":"CiLnuFXG4Bdm","cellId":"rgra31480u3h8nfb7hzj4"}},{"cell_type":"code","source":"# Качаем архив с данными с Yandex Object Storage.\n\n#from cloud_ml.storage.api import Storage\n\n#s3 = Storage.s3(access_key=\"Le9tg70HQEJsoGqjqXH8\", secret_key=\"NV75mCPkC0PEd35ImyDI5vI7p40YGFOYZgkH7moa\")\n# downloading contents of the remote file into the local one\n#s3.get_dir('dl-hse-2021/stanford-drone-dataset-frames/', './stanford-drone-dataset-frames/')","metadata":{"tags":[],"cellId":"bo51i7oqlpdb9m8dwn3spm","trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Задание 1\n**(0.2 балла)** Напишите класс датасет, который будет возвращать картинку и координаты размеченных на ней объектов.","metadata":{"id":"i8b_O3Y94Bdq","cellId":"zrykja70wsr3sw184uq2t7"}},{"cell_type":"code","source":"def get_paths (frames_path, annotations_path):\n    columns = ['track_id', 'xmin', 'ymin', 'xmax', 'ymax', 'frame', 'lost', 'occluded',\n       'generated', 'label', 'frame_idx', 'path']\n    df = pd.DataFrame(columns=columns)\n\n    frames_paths = []\n    annotations_paths = []\n    for i in os.walk(frames_path):\n        frames_paths.append(i)\n    for i in os.walk(annotations_path):\n        annotations_paths.append(i)\n\n    # все пути для кадров    \n    frames_paths_ = []   \n    for address, dirs, files in frames_paths:\n                for file in files:\n                    frames_paths_.append(address+'/'+file)\n\n    for address, dirs, files in annotations_paths:\n                for file in files:\n                    path_ = address+'/'+file\n                    df2 = pd.read_csv(path_, dtype={'xmin': int, 'ymin': int, 'xmax': int, 'ymax': int, 'frame_idx' : str})\n                    a = address+'/'\n                    where = np.where(np.array([a[47:] in frames_paths_[i] for i in range (len(frames_paths_))]) == True)[0]\n                    this_frames_paths = [frames_paths_[i] for i in list(where)]\n                    list1 = []\n                    for i in range (len(list(this_frames_paths))):\n\n                        l = this_frames_paths[i].find('_')\n                        r = this_frames_paths[i].find('.')\n                        list1.append([this_frames_paths[i][l+1:r], this_frames_paths[i]])\n                    this_frames_paths_df = pd.DataFrame(list1, columns = ['frame_idx', 'path'])\n                    this_frames_paths_df = this_frames_paths_df.astype({'frame_idx': str})\n\n                    df = df.append(df2.merge(this_frames_paths_df, on = 'frame_idx'))\n    return df","metadata":{"cellId":"smimwsefd1nnzh2i7vx9bi","trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df_train = get_paths ('stanford-drone-dataset-frames/train/frames/', 'stanford-drone-dataset-frames/train/annotations/')\ndf_val = get_paths ('stanford-drone-dataset-frames/val/frames/', 'stanford-drone-dataset-frames/val/annotations/')","metadata":{"cellId":"bwrcqj62ralfip34oua01","trusted":true},"outputs":[],"execution_count":43},{"cell_type":"code","source":"df_train = df_train.query('lost != 1')\ndf_val = df_val.query('lost != 1')","metadata":{"cellId":"shpmbwm2aremlof68gqdm","trusted":true},"outputs":[],"execution_count":44},{"cell_type":"code","source":"df_train['label'] = df_train['label'].map({'Pedestrian': 1, 'Biker': 2})\ndf_val['label'] = df_val['label'].map({'Pedestrian': 1, 'Biker': 2})","metadata":{"cellId":"ef8rm1k5msu87733fbj21j","trusted":true},"outputs":[],"execution_count":45},{"cell_type":"code","source":"train_frames_paths = df_train['path'].unique()\nval_frames_paths = df_val['path'].unique()","metadata":{"cellId":"4q0bw903txvjrnk5iajfso","trusted":true},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def process_image(image):\n    img = np.asarray(image)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32) / 255.0 # img \\in [0, 1]\n    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n    std =  np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n    img = (img - mean) / std\n    img = img.astype(np.float32)\n       \n    return img","metadata":{"cellId":"j4lf4y3w6wqpkaubzvo82f","trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class StanfordDroneDataset(Dataset):\n    def __init__(self, paths_list, df, transform):\n        self.transform = transform\n        self.paths_list = paths_list\n        self.df = df\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.paths_list)\n    \n    def __getitem__(self, index):\n        img_path = self.paths_list[index]\n        img = cv2.imread(img_path)\n        img = process_image(img)\n        idx = list(np.where(self.df['path'] == self.paths_list[index])[0])\n        annot = self.df.iloc[idx,:]\n        #annot.loc[annot.loc[:,'ymax'] > img.shape[0], 'ymax'] = img.shape[0]\n        #annot.loc[annot.loc[:,'xmax'] > img.shape[1], 'xmax'] = img.shape[1]\n        frame_idx = annot['frame_idx'].unique()[0]\n        category = list(annot['label'])\n        bboxes = list(annot.loc[:,'xmin':'ymax'].itertuples(index=False, name=None))\n        ret = {\"image\": img, \"frame_idx\": frame_idx, \"category\": category, \"bboxes\": bboxes}\n        \n        if self.transform is not None:\n            ret = self.transform(**ret)\n        return ret","metadata":{"cellId":"oc7biaz3ye06kf75gvciv5","trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Задание 2\n(0.2 балла) сделайте подготовку данных на Albumentations, collate_fn и правильный даталоадер, проверьте шейпы выходных тензоров","metadata":{"cellId":"cpt87jos2w6wn3a531w8ig"}},{"cell_type":"code","source":"transform = A.Compose(\n    [A.ShiftScaleRotate(p=0.5), A.RandomCrop(720, 720, always_apply=True), # размеры для кропа меньше минимальных размеров кадра, проверила\n    APT.ToTensorV2(always_apply=True)],\n    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category']),\n)","metadata":{"cellId":"hwhff5q1kyl03nfnleu308b","trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_dataset = StanfordDroneDataset(paths_list = train_frames_paths, df = df_train, transform = transform)\nval_dataset = StanfordDroneDataset(paths_list = val_frames_paths, df = df_val,  transform = transform)","metadata":{"cellId":"4wwhddlxxxahmlugn7um8","trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"train_dataset[1]","metadata":{"cellId":"7cruvep41zfy5hei749rfa","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"{'image': tensor([[[-1.5357, -1.5357, -1.5185,  ..., -0.0116, -0.0116,  0.0056],\n          [-1.5357, -1.5357, -1.4672,  ...,  0.0056,  0.0227,  0.0227],\n          [-1.5357, -1.5357, -1.5870,  ...,  0.0056,  0.0056,  0.0056],\n          ...,\n          [-0.3198, -0.3198, -0.3027,  ..., -0.1999, -0.2171, -0.2342],\n          [-0.3198, -0.3198, -0.3027,  ..., -0.0972, -0.1143, -0.1486],\n          [-0.3198, -0.3198, -0.3027,  ..., -0.0629, -0.0801, -0.1143]],\n \n         [[-1.5805, -1.5630, -1.5455,  ..., -0.2325, -0.2325, -0.2150],\n          [-1.5630, -1.5630, -1.4755,  ..., -0.1800, -0.1625, -0.1625],\n          [-1.5630, -1.5630, -1.5980,  ..., -0.1800, -0.1800, -0.1800],\n          ...,\n          [-0.4601, -0.4601, -0.4426,  ..., -0.4076, -0.4251, -0.4426],\n          [-0.4601, -0.4601, -0.4426,  ..., -0.3025, -0.3200, -0.3550],\n          [-0.4601, -0.4601, -0.4426,  ..., -0.2675, -0.2850, -0.3200]],\n \n         [[-1.1596, -1.1944, -1.1944,  ...,  0.0256,  0.0256,  0.0431],\n          [-1.1944, -1.2119, -1.1770,  ...,  0.0431,  0.0605,  0.0605],\n          [-1.2119, -1.2119, -1.2990,  ...,  0.0431,  0.0431,  0.0431],\n          ...,\n          [-0.3230, -0.3230, -0.3055,  ..., -0.2881, -0.3055, -0.3230],\n          [-0.3230, -0.3230, -0.3055,  ..., -0.1835, -0.2010, -0.2358],\n          [-0.3230, -0.3230, -0.3055,  ..., -0.1487, -0.1661, -0.2010]]]),\n 'frame_idx': '202',\n 'category': [1],\n 'bboxes': [(245.0, 259.0, 282.0, 282.0)]}"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"train_dataset[100]['image'].shape","metadata":{"cellId":"7rfy2ah6hurtmkt7u1ycm","trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"torch.Size([3, 720, 720])"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"def collate_fn(lst):\n    tmp = defaultdict(list)\n    ret = dict()\n    for entry in lst:\n        for k, v in entry.items():\n            tmp[k].append(v)\n    \n    for k, v in tmp.items():\n        if isinstance(v[0], np.ndarray):\n            for vv in v:\n                print(vv.shape)\n            v = np.concatenate(v, 0)\n        if isinstance(v[0], torch.Tensor):\n            v = torch.cat(v, 0)\n        ret[k] = v\n    return ret\n\n\ndl = DataLoader(train_dataset, shuffle=True, batch_size=100, collate_fn=collate_fn)\n\nbatch = next(iter(dl))","metadata":{"cellId":"qkqbk7phxic0yucdhajz2uf","trusted":true},"outputs":[],"execution_count":50},{"cell_type":"code","source":"print(batch['image'][0])\nprint(batch['frame_idx'][0])\nprint(batch['category'][0])\nprint(batch['bboxes'][0])","metadata":{"cellId":"wukbav6gm2yajk6dokzl","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([[-0.4451, -0.4312, -0.4493,  ...,  0.2282,  0.2250,  0.2111],\n        [-0.4498, -0.4556, -0.4616,  ...,  0.2282,  0.2164,  0.2111],\n        [-0.4300, -0.4429, -0.4515,  ...,  0.2228,  0.2111,  0.2111],\n        ...,\n        [-0.1620, -0.1571, -0.1357,  ..., -0.5042, -0.4734, -0.4504],\n        [-0.1654, -0.1400, -0.1518,  ..., -0.5135, -0.4798, -0.4573],\n        [-0.1528, -0.1389, -0.1673,  ..., -0.5003, -0.4226, -0.4343]])\n1370\n[1, 1]\n[(488.49873019738646, 157.9203529289657, 558.801349015002, 235.90421368847342), (144.04283966914602, 7.084604909225618, 210.6680629451464, 85.99981529225352)]\n"}],"execution_count":51},{"cell_type":"markdown","source":"## Задание 3\n(0.4 балла) Приготовьте модель для детекции, проверьте, что все правильно отрабатывает.","metadata":{"cellId":"6lr7eb86d1lnqffg44op4"}},{"cell_type":"code","source":"","metadata":{"cellId":"ymg2bm4gz96v33qvnaxsg"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"8ktskkkvrlf8pxdb4zldaa"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"3nxofoavymvcwu2fnqsds"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"cellId":"behn9a21zy9jg5zdg9259e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Задание 4\n(0.2 балла) натренируйте модель:\n- Убедитесь, что она учится,\n- Проверьте, что на выходе что-то адекватное.\n\nТрейнер можно взять с любого занятия.\n","metadata":{"cellId":"pcs2rarvumduuxampf2vu"}},{"cell_type":"code","source":"","metadata":{"cellId":"7nc8gcp4vce662dl6uw9wk"},"outputs":[],"execution_count":null}]}
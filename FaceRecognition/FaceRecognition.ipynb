{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","version":"3.7.7","file_extension":".py","pygments_lexer":"ipython3"},"notebookId":"0afb9840-d068-4d1e-a5e0-09475c6cdcc2"},"cells":[{"cell_type":"markdown","source":["# CV part one\n","\n","В этой тетрадке мы рассмотрим задачу распознавания лиц на примере датасета [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)\n","\n","**Предполагаем, что ноутбук запущен внутри Yandex DataSphere**"],"metadata":{"cellId":"38lqsetzmcbfb8lk82ubeg"}},{"cell_type":"code","source":["#!M\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import cv2\n","\n","import torch\n","import torch.nn as nn\n","# import torch.nn.functional as F\n","# from torch import optim\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","from torchvision.models import resnet34\n","# from torch.utils.tensorboard import SummaryWriter\n","from sklearn.metrics.pairwise import euclidean_distances\n","import torch.nn.functional as F\n","from collections import Counter\n","from torch.utils.data import TensorDataset"],"metadata":{"cellId":"212z2memf5cmplpkb5eb0s"},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Data\n","\n","Качаем архив с данными с Yandex Object Storage и распаковываем в текущую папку.\n","\n","Структура архива:\n","- /celeba_data/\n","    - train.csv\n","    - val.csv\n","    - images/{image}.jpg\n","\n","CSV файлы содержат название файла (`image`) и его лейбл (`label`)."],"metadata":{"cellId":"rgra31480u3h8nfb7hzj4"}},{"cell_type":"code","source":["#!M\n","from cloud_ml.storage.api import Storage\n","\n","s3 = Storage.s3(access_key=\"Le9tg70HQEJsoGqjqXH8\", secret_key=\"NV75mCPkC0PEd35ImyDI5vI7p40YGFOYZgkH7moa\")\n","# downloading contents of the remote file into the local one\n","s3.get('dl-hse-2021/celeba_data.zip', './celeba_data.zip')"],"metadata":{"cellId":"bo51i7oqlpdb9m8dwn3spm"},"outputs":[{"timestamp":1616164506531,"text":"\r","overwrite":0,"output_type":"stream","name":"stdout"}],"execution_count":2},{"cell_type":"code","source":["#!:bash\n","unzip -q ./celeba_data.zip -d ./ && rm celeba_data.zip"],"metadata":{"cellId":"mbja58q90lq40q6jedx575"},"outputs":[{"timestamp":1616164561609,"text":"replace ./celeba_data/images/052628.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: error:  invalid response [#]\nreplace ./celeba_data/images/052628.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n(EOF or read error, treating as \"[N]one\" ...)\n","overwrite":0,"output_type":"stream","name":"stderr"}],"execution_count":3},{"cell_type":"markdown","source":["## Задание 1\n","**(0.2 балла)** Напишите класс датасет, который будет возвращать картинку и ее лейбл."],"metadata":{"cellId":"zrykja70wsr3sw184uq2t7"}},{"cell_type":"code","source":["#!M\n","class CelebADataset(Dataset):\n","    def __init__(self, images_dir_path: str,\n","                 description_csv_path: str):\n","        super().__init__()\n","        \n","        self.images_dir_path = images_dir_path\n","        self.description_df = pd.read_csv(description_csv_path,\n","                                           dtype={'image_name': str, 'label': int})\n","        \n","    def __len__(self):\n","        return self.data.shape[0]\n","    \n","    def __getitem__(self, item):\n","        img, label = self.description_df.iloc[item, :]\n","        \n","        img_path = Path(self.images_dir_path, f'{img}')\n","        img = cv2.imread(str(img_path.resolve()))\n","        \n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n","       \n","        img = img.astype(np.float32) / 255.0 # img \\in [0, 1]\n","        mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n","        std =  np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n","        img = (img - mean) / std\n","        img = img.astype(np.float32)\n","        img = np.transpose(img, [2, 0, 1])[None, ...]\n","        img = torch.tensor(img)\n","                \n","        return dict(\n","            sample=img,\n","            label=label,\n","        )"],"metadata":{"cellId":"oc7biaz3ye06kf75gvciv5"},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#!M\n","train = CelebADataset(images_dir_path='celeba_data/images/',\n","                           description_csv_path='celeba_data/train.csv')\n","val = CelebADataset(images_dir_path='celeba_data/images/',\n","                         description_csv_path='celeba_data/val.csv')"],"metadata":{"cellId":"8kyjv3ffzvbkg18c6ciuxb"},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Задание 2\n","**(0.2 балла)** Напишите функцию, которая будет считать метрику top-n accuracy.\n","\n","$$TopN \\ Accuracy = \\frac{Number \\ of \\ objects \\ with \\ correct \\ answer \\ among \\ topN \\ predictions}{Total \\ number \\ of \\ objects}$$\n","\n","*Example:*\n","\n","![image](https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-ae746981c7a437b7e1fc2831e5d76d57_l3.svg)  \n","$Top3 \\ Accuracy = \\frac{4}{5} = 0.8$\n","\n","*Hint:* Для каждого объекта выбираем `n` наиболее уверенных предсказаний. Если среди них есть правильный ответ, то увеличиваем числитель и знаменатель на единицу, иначе увеличиваем только знаменатель."],"metadata":{"cellId":"42liqfoavi30iheofhu1m2"}},{"cell_type":"code","source":["#!M\n","def top_n_accuracy(preds: np.ndarray,\n","                   targets: np.ndarray,\n","                   n_size: int) -> float:\n","    preds = preds.T[0:n_size].T\n","    num = 0\n","    \n","    for i in range(len(targets)):\n","        if targets[i] in preds[i]:\n","            num += 1\n","            \n","    return num/len(targets)"],"metadata":{"cellId":"6btuqm12a69cz6fgag1coi"},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Задание 3\n","**(0.2 балла)** Решите задачу без дообучения.\n","\n","*Step-by-step:*\n","1. Инициализируйте предобученную сетку (`backbone`).\n","1. Прогоните через нее все картинки из валидационного датасета и сложите полученные эмбеддинги в массив.\n","1. Для каждого вектора найдите ближайшие к нему векторы и отсортируйте их по расстоянию (cosine, euclidian, ...). Лейблы соседних векторов будут предсказаниями для текущего вектора.\n","1. Оставьте топ-5 уникальных предсказаний.\n","1. Посчитайте и выведите метрики:\n","    1. top-1 accuracy\n","    1. top-5 accuracy\n","\n","*Вопросы:*\n","1. Зачем мы заменяем последний линейный слой на `Identity` ?\n","1. Зачем используем на сетке метод `eval` ?\n","\n","*Hints:*\n","1. Для расчета попарных расстояний лучше не использовать циклы, а считать все в матрицах. Описание подхода к расчету L2 расстояний: [link](https://math.stackexchange.com/questions/3147549/compute-the-pairwise-euclidean-distance-matrix)\n","1. Так можно использовать sklearn реализации: [link](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise)\n","1. Для получения top-k предсказаний не обязательно сортировать весь массив."],"metadata":{"cellId":"jgagu11ef1hwupvrwohxk"}},{"cell_type":"markdown","source":["Ответы на вопросы:\n","1. Потому что иначе мы получим вероятности принадлежности к классу. А нас интересуют не вероятности, а возможность узнать лейблы N ближайших соседей, чтобы определить есть ли истинный лейбл среди них.\n","2. Чтобы прогнать тензоры черезь сеть без ее тренировки, нужен метод eval. Если dropout и batch normalization слои не в режиме eval, это приводит к искажению результатов.  "],"metadata":{"cellId":"p4ft7uq7o49nl9nz86l8e"}},{"cell_type":"code","source":["#!M\n","backbone = resnet34(pretrained=True)\n","backbone.fc = nn.Identity()\n","backbone = backbone.eval()\n","\n","val_loader = DataLoader(val, shuffle=False, batch_size=1)\n","\n","vectors = []\n","labels = []\n","with torch.no_grad():\n","    for data in val_loader.dataset:\n","        x = data['label']\n","        labels.append(x)\n","        y =  backbone(data['sample'])\n","        y = np.array(y).flatten()\n","        vectors.append(y)\n","        \n","distances = euclidean_distances(vectors)\n","np.fill_diagonal(distances, 10**10)\n","\n","preds = []\n","for j in range(distances.shape[0]):\n","    distance_label = dict(zip(distances[j],labels))\n","    list_keys = list(distance_label.keys())\n","    list_keys.sort()\n","    k = 0\n","    for i in list_keys:\n","        k+=1\n","        preds.append(distance_label[i])\n","        if k > 30:\n","            break\n","            \n","n = np.array(labels).shape[0]\n","preds = np.array(preds).reshape((n, k))\n","\n","top5 = []\n","for i in range(n):\n","    top5_1 = []\n","    for j in preds[i]:\n","        \n","        if j not in top5_1:\n","            top5_1.append(j)\n","        if len(top5_1) == 5:\n","            break\n","    top5.append(top5_1)\n","\n","print('top5', top_n_accuracy(np.array(top5), np.array(labels), 5))\n","print('top1', top_n_accuracy(np.array(top5), np.array(labels), 1))"],"metadata":{"cellId":"12gucdwzunnmqydpy2s2ur"},"outputs":[{"timestamp":1616165834436,"text":"top5 0.28952534353450443\ntop1 0.18125534806462978\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616165834713,"text":"/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'str'>\nWalking trough too many objects\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'dict'> with size: 19784\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 19784\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 19867\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 19867\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 19867\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n","overwrite":0,"output_type":"stream","name":"stderr"}],"execution_count":9},{"cell_type":"markdown","source":["## Задание 4\n","**(0.4 балла)** Решите задачу с дообучением на эмбеддингах.\n","\n","*Step-by-step:*\n","1. Напишите небольшую сетку произвольной архитектуры, которая будет использовать эмбеды, выдаваемые `backbone` сетью.\n","1. Напишите класс Dataset, который будет возвращать эмбединг и лейбл.\n","1. Напишите класс Sampler [PyTroch docs](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler), который будет отвечать за правильность сбора тренировочных батчей: якорный пример, позитивный, негативный.\n","1. Обучите ее на тренировочном датасете:\n","    1. Лосс -- [triplet loss](https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html).\n","    1. Метрика -- top-5 accuracy.\n","1. Посчитайте top-1 и top-5 accuracy на валидации. Насколько сильно они отличаются от того, что получилось в предыдущем задании?\n","\n","\n","*Hints:*\n","1. Убедитесь, что у каждого лейбла есть как минимум 2 примера, иначе не получится достать позитивный пример.\n","1. Лучше предварительно прогнать все картинки из трейна и сохранить полученные эмбеддинги, чтобы при обучении сети грузить только эмбеды (векторы)."],"metadata":{"cellId":"qyxj31ya2qg7ht7gehbftk"}},{"cell_type":"code","source":["#!M\n","backbone = resnet34(pretrained=True)\n","backbone.fc = nn.Identity()\n","backbone = backbone.eval()\n","\n","train_loader = DataLoader(train, shuffle=False, batch_size=1)\n","\n","train_vectors = []\n","train_labels = []\n","with torch.no_grad():\n","    for data in train_loader.dataset:\n","        x = data['label']\n","        train_labels.append(x)\n","        y =  backbone(data['sample'])\n","        y = np.array(y).flatten()\n","        train_vectors.append(y)"],"metadata":{"cellId":"ahsr684dszynlxn62csks"},"outputs":[{"timestamp":1616183087449,"text":"/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 162770\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 162770\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 19867\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'str'>\nWalking trough too many objects\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'str'>\nWalking trough too many objects\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 19867\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n","overwrite":0,"output_type":"stream","name":"stderr"}],"execution_count":12},{"cell_type":"code","source":["#!XL\n","# создание датасетов \n","train_dataset = TensorDataset(torch.tensor(train_vectors), torch.tensor(train_labels))\n","val_dataset = TensorDataset(torch.tensor(vectors), torch.tensor(labels))       \n","    \n","#Вместо самплера\n","#------------------------------------\n","def get_anchor_positive_mask(labels):\n","\n","    # Check that i and j are distinct\n","    indices_equal = torch.eye(labels.size(0)).bool()\n","    indices_not_equal = ~indices_equal\n","\n","    # Check if labels[i] == labels[j]\n","    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n","    labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)\n","\n","    return labels_equal & indices_not_equal\n","\n","\n","def get_anchor_negative_mask(labels):\n","\n","    # Check if labels[i] != labels[k]\n","    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n","\n","    return ~(labels.unsqueeze(0) == labels.unsqueeze(1))\n","\n","def get_triplet_mask(labels):\n","\n","    mask_anchor_positive = get_anchor_positive_mask(labels)\n","    mask_anchor_negative = get_anchor_negative_mask(labels)\n","\n","    return mask_anchor_positive.unsqueeze(2) & mask_anchor_negative.unsqueeze(1)\n","\n","def batch_triplet_loss(embeddings, labels):\n","    \n","    pairwise_dist = torch.cdist(embeddings, embeddings, p = 2)\n","    \n","    mask_anchor_positive = get_anchor_positive_mask(labels).float()\n","    anchor_positive_dist = pairwise_dist * mask_anchor_positive\n","    hardest_positive_dist, _ = anchor_positive_dist.max(1, keepdim=True)\n","\n","    \n","    mask_anchor_negative = get_anchor_negative_mask(labels).float()\n","    anchor_negative_dist = pairwise_dist + 999. * (1.0 - mask_anchor_negative)\n","    hardest_negative_dist, _ = anchor_negative_dist.min(1, keepdim=True)\n","\n","    triplet_loss = hardest_positive_dist - hardest_negative_dist + params[\"margin\"]\n","    \n","    triplet_loss[triplet_loss < 0] = 0\n","    return torch.mean(triplet_loss)\n","#------------------------------------    \n","\n","class convEmbedding(nn.Module):\n","    def __init__(self, c_dim = 1):\n","        super().__init__()\n","\n","        self.embedding = nn.Sequential(nn.Flatten(), nn.Linear(512, 2048), nn.ReLU(), nn.Linear(2048, 512))\n","\n","\n","    def forward(self, input_tensor):\n","        x = self.embedding(input_tensor)\n","        return F.normalize(x, p =2, dim = 1)\n","    \n","model = convEmbedding()\n","\n","def collate(batch):\n","    data = [item[0].view(1, 16,32) for item in batch]\n","    data = torch.stack(data).float()\n","\n","    target = [item[1] for item in batch]\n","    target = torch.LongTensor(target)\n","    return data, target\n","\n","def train_one_batch(model, x, y):\n","    x = x\n","\n","    batch_embeddings = model(x)\n","    loss = batch_triplet_loss(batch_embeddings, y)\n","    \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    return loss.item()\n","\n","\n","params = {\"learning_rate\": 0.01, \"margin\": 0.5, \"batch_size\":  128, \"epochs\": 50} \n","\n","trainloader = DataLoader(train_dataset, batch_size = params[\"batch_size\"], collate_fn = collate, drop_last = True)\n","steps_per_epoch  = train_dataset.__len__()// params[\"batch_size\"]\n","\n","model = convEmbedding()\n","optimizer = torch.optim.SGD(model.parameters(), params['learning_rate'])\n","\n","  \n","train_losses = []\n","model.train()\n","for epoch in range(1, params[\"epochs\"]+1):\n","    print('-' * 10)\n","    print('Epoch {}/{}\\t{} batches'.format(epoch, params[\"epochs\"], steps_per_epoch))\n","    \n","    curr_loss = []\n","    for step, (x, y) in enumerate(trainloader):\n","        loss = train_one_batch(model, x, y)\n","        curr_loss.append(loss)\n","        print('\\rprogress {:6.1f} %\\tloss {:8.8f}'.format(100*(step+1)/steps_per_epoch, np.mean(curr_loss)), end = \"\")\n","        \n","    train_losses.append(np.mean(curr_loss))\n","    print('\\rprogress {:6.1f} %\\tloss {:8.8f}'.format(100*(step+1)/steps_per_epoch, np.mean(curr_loss)))\n","   \n","\n","model.eval()\n","\n","val_loader = DataLoader(val_dataset, batch_size = 128, collate_fn = collate)\n","\n","#!M\n","X = []; val_labels = []\n","for x, label in val_loader:\n","    batch_embedings = model(x)\n","    X.append(batch_embedings.cpu().detach().numpy())\n","    val_labels.append(label.numpy())\n","    \n","\n","X = np.concatenate(X)\n","val_labels = np.concatenate(val_labels)\n","distances = euclidean_distances(X)\n","np.fill_diagonal(distances, 10**10)\n","\n","preds = []\n","for j in range(distances.shape[0]):\n","    distance_label = dict(zip(distances[j],val_labels))\n","    list_keys = list(distance_label.keys())\n","    list_keys.sort()\n","    k = 0\n","    for i in list_keys:\n","        k+=1\n","        preds.append(distance_label[i])\n","        if k > 30:\n","            break\n","            \n","n = np.array(val_labels).shape[0]\n","preds = np.array(preds).reshape((n, k))\n","\n","top5 = []\n","for i in range(n):\n","    top5_1 = []\n","    for j in preds[i]:\n","        \n","        if j not in top5_1:\n","            top5_1.append(j)\n","        if len(top5_1) == 5:\n","            break\n","    top5.append(top5_1)\n","print('----------')\n","print('top5', top_n_accuracy(np.array(top5), np.array(val_labels), 5))\n","print('top1', top_n_accuracy(np.array(top5), np.array(val_labels), 1))"],"metadata":{"cellId":"vw6ssqb2ahbhxbi8zythud"},"outputs":[{"timestamp":1616318173628,"text":"----------\nEpoch 1/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318188165,"text":"\rprogress  100.0 %\tloss 0.01196944\n----------\nEpoch 2/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318201241,"text":"\rprogress  100.0 %\tloss 0.01140376\n----------\nEpoch 3/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318214344,"text":"\rprogress  100.0 %\tloss 0.01121165\n----------\nEpoch 4/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318226743,"text":"\rprogress  100.0 %\tloss 0.01107452\n----------\nEpoch 5/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318239580,"text":"\rprogress  100.0 %\tloss 0.01096592\n----------\nEpoch 6/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318251874,"text":"\rprogress  100.0 %\tloss 0.01087375\n----------\nEpoch 7/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318264120,"text":"\rprogress  100.0 %\tloss 0.01079364\n----------\nEpoch 8/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318276564,"text":"\rprogress  100.0 %\tloss 0.01072195\n----------\nEpoch 9/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318289468,"text":"\rprogress  100.0 %\tloss 0.01065698\n----------\nEpoch 10/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318302135,"text":"\rprogress  100.0 %\tloss 0.01059765\n----------\nEpoch 11/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318314371,"text":"\rprogress  100.0 %\tloss 0.01054226\n----------\nEpoch 12/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318327191,"text":"\rprogress  100.0 %\tloss 0.01049032\n----------\nEpoch 13/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318339592,"text":"\rprogress  100.0 %\tloss 0.01044151\n----------\nEpoch 14/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318352106,"text":"\rprogress  100.0 %\tloss 0.01039564\n----------\nEpoch 15/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318364960,"text":"\rprogress  100.0 %\tloss 0.01035188\n----------\nEpoch 16/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318377670,"text":"\rprogress  100.0 %\tloss 0.01030990\n----------\nEpoch 17/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318390312,"text":"\rprogress  100.0 %\tloss 0.01027004\n----------\nEpoch 18/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318403227,"text":"\rprogress  100.0 %\tloss 0.01023200\n----------\nEpoch 19/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318415494,"text":"\rprogress  100.0 %\tloss 0.01019530\n----------\nEpoch 20/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318428241,"text":"\rprogress  100.0 %\tloss 0.01016021\n----------\nEpoch 21/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318440677,"text":"\rprogress  100.0 %\tloss 0.01012621\n----------\nEpoch 22/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318453599,"text":"\rprogress  100.0 %\tloss 0.01009307\n----------\nEpoch 23/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318465578,"text":"\rprogress  100.0 %\tloss 0.01006116\n----------\nEpoch 24/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318477578,"text":"\rprogress  100.0 %\tloss 0.01003004\n----------\nEpoch 25/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318489621,"text":"\rprogress  100.0 %\tloss 0.00999983\n----------\nEpoch 26/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318501450,"text":"\rprogress  100.0 %\tloss 0.00997009\n----------\nEpoch 27/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318513869,"text":"\rprogress  100.0 %\tloss 0.00994132\n----------\nEpoch 28/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318526096,"text":"\rprogress  100.0 %\tloss 0.00991303\n----------\nEpoch 29/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318538105,"text":"\rprogress  100.0 %\tloss 0.00988540\n----------\nEpoch 30/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318550791,"text":"\rprogress  100.0 %\tloss 0.00985839\n----------\nEpoch 31/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318563226,"text":"\rprogress  100.0 %\tloss 0.00983191\n----------\nEpoch 32/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318575408,"text":"\rprogress  100.0 %\tloss 0.00980591\n----------\nEpoch 33/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318588465,"text":"\rprogress  100.0 %\tloss 0.00978014\n----------\nEpoch 34/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318600450,"text":"\rprogress  100.0 %\tloss 0.00975537\n----------\nEpoch 35/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318612455,"text":"\rprogress  100.0 %\tloss 0.00973075\n----------\nEpoch 36/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318624718,"text":"\rprogress  100.0 %\tloss 0.00970654\n----------\nEpoch 37/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318637341,"text":"\rprogress  100.0 %\tloss 0.00968287\n----------\nEpoch 38/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318649829,"text":"\rprogress  100.0 %\tloss 0.00965931\n----------\nEpoch 39/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318662474,"text":"\rprogress  100.0 %\tloss 0.00963618\n----------\nEpoch 40/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318675103,"text":"\rprogress  100.0 %\tloss 0.00961335\n----------\nEpoch 41/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318687598,"text":"\rprogress  100.0 %\tloss 0.00959092\n----------\nEpoch 42/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318699581,"text":"\rprogress  100.0 %\tloss 0.00956845\n----------\nEpoch 43/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318712017,"text":"\rprogress  100.0 %\tloss 0.00954679\n----------\nEpoch 44/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318724025,"text":"\rprogress  100.0 %\tloss 0.00952480\n----------\nEpoch 45/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318735638,"text":"\rprogress  100.0 %\tloss 0.00950339\n----------\nEpoch 46/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318747869,"text":"\rprogress  100.0 %\tloss 0.00948203\n----------\nEpoch 47/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318759889,"text":"\rprogress  100.0 %\tloss 0.00946102\n----------\nEpoch 48/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318772366,"text":"\rprogress  100.0 %\tloss 0.00944023\n----------\nEpoch 49/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318784810,"text":"\rprogress  100.0 %\tloss 0.00941961\n----------\nEpoch 50/50\t1271 batches\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616318796619,"text":"\rprogress  100.0 %\tloss 0.00939906\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616319347300,"text":"----------\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616319347796,"text":"top5 0.3264710323652288\ntop1 0.20299994966527407\n","overwrite":0,"output_type":"stream","name":"stdout"},{"timestamp":1616319350421,"text":"/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 19867\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'dict'> with size: 19767\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 19767\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 162770\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 19867\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 162770\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 1271\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n/kernel/lib/python3.7/site-packages/ipystate/state.py:135: UserWarning: Skipping walk through <class 'list'> with size: 19867\nUse %enable_full_walk to serialize all variables correctly\n  {name: self._state[name] for name in self._state.varnames() if not self._skip_variable(name)}\n","overwrite":0,"output_type":"stream","name":"stderr"}],"execution_count":60},{"cell_type":"code","source":["# your code must be before example"],"metadata":{"cellId":"5lr9yul9izkgcxl55jdqjv"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Sampler (simple example)\n","\n","В блоках ниже реализован пример датасета и сэмлера, который возвращает индексы для триплет лосса.\n","\n","Датасет написан топорно, но основная логика следующая. Если ему на вход приходит `int`, то он возвращает название картинки (`img_name`) и ее лейбл (`img_label`). Если же приходит нечто длиной 3, то он возвращает 3 названия картинок, соответственно. В нашем случае это будет три картинки с двумя одинаковыми лейблами и одним другим: anchor, positive, negative.  \n","Сэмплер `SimpleTripletSampler`, в свою очередь, отвечает за формирование и поставку в датасет индексов триплетов.\n","\n","Датасет и сэмлер объединяются внутри даталоадера.\n","\n","*Hint:* Код написан только лишь для примера, поэтому логика возвращения триплетов может быть неверной."],"metadata":{"cellId":"qmk3j5diof9try6p5pv3kh"}},{"cell_type":"code","source":["class SimpleDataset(Dataset):\n","    def __init__(self, img_names: np.ndarray,\n","                 img_labels: np.ndarray):\n","        if len(img_names) != len(img_labels):\n","            raise ValueError('img_names and img_labels must have equal number of elements')\n","\n","        self.img_names = img_names\n","        self.img_labels = img_labels\n","\n","    def __len__(self):\n","        return len(self.img_names)\n","    \n","    def __getitem__(self, idx):\n","        if isinstance(idx, int):\n","            img_name = self.img_names[idx]\n","            img_label = self.img_labels[idx]\n","            \n","            return img_name, img_label\n","        else:\n","            assert len(idx) == 3\n","            \n","            anc_idx, pos_idx, neg_idx = idx\n","            anc_img_name = self.img_names[anc_idx]\n","            pos_img_name = self.img_names[pos_idx]\n","            neg_img_name = self.img_names[neg_idx]\n","\n","            return anc_img_name, pos_img_name, neg_img_name\n","\n","\n","class SimpleTripletSampler(Sampler):\n","    def __init__(self, dataset: Dataset):\n","        super().__init__(dataset)\n","\n","        self.dataset = dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __iter__(self):\n","        for anchor_idx in range(len(self.dataset)):\n","            positive_idx = self._mine_positive(anchor_idx)\n","            negative_idx = self._mine_negative(anchor_idx)\n","\n","            yield anchor_idx, positive_idx, negative_idx\n","\n","    def _mine_positive(self, anchor_idx: int):\n","        labels: np.ndarray = self.dataset.img_labels\n","\n","        anchor_label = labels[anchor_idx]\n","        pos_idxs = np.nonzero(labels == anchor_label)[0]\n","        pos_idx = np.random.choice(pos_idxs)\n","\n","        return pos_idx\n","\n","    def _mine_negative(self, anchor_idx: int):\n","        labels: np.ndarray = self.dataset.img_labels\n","\n","        anchor_label = labels[anchor_idx]\n","        neg_idxs = np.nonzero(labels != anchor_label)[0]\n","        neg_idx = np.random.choice(neg_idxs)\n","\n","        return neg_idx"],"metadata":{"cellId":"03biv2ae8tcdrmqcc7q0p"},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ex_size = 100\n","np.random.seed(42)\n","\n","# в нашем примере названием картинки будет выступать число от 0 до 99, а лейблом число от 0 до 4.\n","ex_dataset = SimpleDataset(img_names=np.arange(ex_size),\n","                           img_labels=np.random.randint(0, 5, size=ex_size))\n","ex_sampler = SimpleTripletSampler(dataset=ex_dataset)\n","\n","ex_loader = DataLoader(dataset=ex_dataset, batch_size=10, sampler=ex_sampler)"],"metadata":{"cellId":"f6tcl0cqlac5e4eqk52ol"},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# В этой ячейке мы дергаем первый батч с названиями картинок и достаем их лейблы, \n","#  чтобы проверить действительно ли у них одинаковые или разные лейблы.\n","# Для тренировки сети с триплет лоссом сами лейблы нам не нужны будут.\n","#  Главное чтобы триплеты картинок формировались правильно: anchor, positive, negative\n","\n","ex_batch = next(iter(ex_loader))\n","\n","ex_batch_anc_labels = ex_dataset.img_labels[ex_batch[0]]\n","ex_batch_pos_labels = ex_dataset.img_labels[ex_batch[1]]\n","ex_batch_neg_labels = ex_dataset.img_labels[ex_batch[2]]"],"metadata":{"cellId":"8z6lx8z9q1s0npxi2edxuz"},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print('All anchor and positive labels are equal:', np.all(ex_batch_anc_labels == ex_batch_pos_labels))\n","print('Any of anchor and negative labels are equal:', np.any(ex_batch_anc_labels == ex_batch_neg_labels))"],"metadata":{"cellId":"vxcfeoyptv0eboo06ouk96"},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#!S\n",""],"metadata":{"cellId":"k3zafwb0ze1ddkt4erc3lh"},"outputs":[],"execution_count":0}]}
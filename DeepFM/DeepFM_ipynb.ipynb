{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Копия блокнота \"DeepFM.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngYw8JUVNyxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d98003b-3a12-48ff-e167-a2a505ec1147"
      },
      "source": [
        "! git clone https://github.com/Firyuza/SiriusDL.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SiriusDL'...\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects: 100% (195/195), done.\u001b[K\n",
            "remote: Compressing objects: 100% (157/157), done.\u001b[K\n",
            "remote: Total 195 (delta 50), reused 168 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (195/195), 39.63 MiB | 27.33 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g94C-16e5zhz"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "! pip install pytorch_lightning\n",
        "clear_output()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myxpS7DHNrTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df5826b-6944-43a4-dbb3-432e32551b1d"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from pytorch_lightning.metrics import Accuracy\n",
        "\n",
        "from SiriusDL.week08.deepFM.network import DeepFMNet\n",
        "from SiriusDL.week08.deepFM.data_loader import CustomDataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MABv12dkdYYG"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P53FsQ3DNrTi"
      },
      "source": [
        "EPOCHS = 20\n",
        "EMBEDDING_SIZE = 5\n",
        "BATCH_SIZE = 512\n",
        "NROF_LAYERS = 3\n",
        "NROF_NEURONS = 50\n",
        "DEEP_OUTPUT_SIZE = 50\n",
        "NROF_OUT_CLASSES = 1\n",
        "LEARNING_RATE = 3e-4\n",
        "TRAIN_PATH = '/content/SiriusDL/week08/data/train_adult.pickle'\n",
        "VALID_PATH = '/content/SiriusDL/week08/data/valid_adult.pickle'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kH6SWrIB55c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y5ZgqWRB213"
      },
      "source": [
        "embedding_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race',\n",
        "                                 'sex', 'native-country']\n",
        "nrof_emb_categories = {}\n",
        "unique_categories = {}\n",
        "\n",
        "with open('/content/SiriusDL/week08/data/train_adult.pickle', 'rb') as f:\n",
        "    data, _, _ = pickle.load(f)\n",
        "\n",
        "for cat in embedding_columns:\n",
        "    nrof_unique = np.unique(data[cat].values.astype(np.str))\n",
        "    # data.groupby(cat).agg({cat: 'count'})\n",
        "    unique_categories[cat] = nrof_unique\n",
        "    nrof_emb_categories[cat] = len(nrof_unique)\n",
        "    data[cat + '_cat'] = [np.where(nrof_unique == val)[0][0] for i, val in enumerate(data[cat].values.astype(np.str))]\n",
        "\n",
        "data.dropna(axis=0,inplace=True)\n",
        "min_age = data.age.min()  \n",
        "max_age = data.age.max()\n",
        "step = (max_age - min_age)/10\n",
        "\n",
        "feature_list = data['age'].unique()\n",
        "\n",
        "for i in feature_list:\n",
        "    mask = (data['age'] == i)\n",
        "    g = np.floor((i - min_age)/ step)\n",
        "    data.loc[mask, 'age' + '_bin'] = g\n",
        "\n",
        "data = data.drop(columns=[\"age\"])  \n",
        "data.age_bin = data.age_bin.astype(int)\n",
        "with open('/content/SiriusDL/week08/data/train_adult.pickle', 'wb') as f:\n",
        "    pickle.dump([data, nrof_emb_categories, unique_categories], f)\n",
        "\n",
        "\n",
        "with open('/content/SiriusDL/week08/data/valid_adult.pickle', 'rb') as f:\n",
        "    data, _, _ = pickle.load(f)\n",
        "\n",
        "for cat in embedding_columns:\n",
        "    data[cat + '_cat'] = [np.where(unique_categories[cat] == val)[0][0] for i, val in enumerate(data[cat].values.astype(np.str))]\n",
        "data.dropna(axis=0,inplace=True)\n",
        "feature_list = data['age'].unique()\n",
        "\n",
        "for i in feature_list:\n",
        "    mask = (data['age'] == i)\n",
        "    g = np.floor((i - min_age)/ step)\n",
        "    data.loc[mask, 'age' + '_bin'] = g\n",
        "\n",
        "data = data.drop(columns=[\"age\"])  \n",
        "data.age_bin = data.age_bin.astype(int)\n",
        "with open('/content/SiriusDL/week08/data/valid_adult.pickle', 'wb') as f:\n",
        "    pickle.dump([data, nrof_emb_categories, unique_categories], f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqut55bpCbFd"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWUMjL_7DId8"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset_path):\n",
        "        with open(dataset_path, 'rb') as f:\n",
        "            data, self.nrof_emb_categories, self.unique_categories = pickle.load(f)\n",
        "\n",
        "        self.embedding_columns = ['workclass_cat', 'education_cat', 'marital-status_cat', 'occupation_cat',\n",
        "                                  'relationship_cat', 'race_cat',\n",
        "                                  'sex_cat', 'native-country_cat']\n",
        "        self.nrof_emb_categories = {key + '_cat': val for key, val in self.nrof_emb_categories.items()}\n",
        "        self.numeric_columns = ['age_bin', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
        "                                'hours-per-week']\n",
        "\n",
        "        self.columns = self.embedding_columns + self.numeric_columns\n",
        "\n",
        "        self.X = data[self.columns].reset_index(drop=True)\n",
        "        self.y = np.asarray([0 if el == '<50k' else 1 for el in data['salary'].values], dtype=np.int32)\n",
        "\n",
        "        return\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        row = self.X.take([idx], axis=0)\n",
        "\n",
        "        row = {col: torch.tensor(row[col].values, dtype=torch.float32) for i, col in enumerate(self.columns)}\n",
        "\n",
        "        return row, self.y[idx]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0KKBFu9NrTj"
      },
      "source": [
        "class DeepFM:\n",
        "    def __init__(self):\n",
        "        self.train_dataset = CustomDataset(TRAIN_PATH)\n",
        "        self.val_dataset = CustomDataset(VALID_PATH)\n",
        "        self.train_loader = data_utils.DataLoader(dataset=self.train_dataset,\n",
        "                                                  batch_size=BATCH_SIZE, shuffle=True)\n",
        "        self.val_loader = data_utils.DataLoader(dataset=self.val_dataset,\n",
        "                                                  batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        self.build_model()\n",
        "\n",
        "        self.log_params()\n",
        "\n",
        "        self.train_writer = SummaryWriter('./logs/train')\n",
        "        self.valid_writer = SummaryWriter('./logs/valid')\n",
        "        self.train_writer.add_text('LEARNING_RATE', str(LEARNING_RATE))\n",
        "        self.train_writer.add_text('BATCH_SIZE', str(BATCH_SIZE))\n",
        "\n",
        "        return\n",
        "\n",
        "    def build_model(self):\n",
        "        self.network = DeepFMNet(nrof_cat=self.train_dataset.nrof_emb_categories, emb_dim=EMBEDDING_SIZE,\n",
        "                                 emb_columns=self.train_dataset.embedding_columns,\n",
        "                                 numeric_columns=self.train_dataset.numeric_columns,\n",
        "                                 nrof_layers=NROF_LAYERS, nrof_neurons=NROF_NEURONS,\n",
        "                                 output_size=DEEP_OUTPUT_SIZE,\n",
        "                                 nrof_out_classes=NROF_OUT_CLASSES)\n",
        "\n",
        "        self.loss = torch.nn.BCEWithLogitsLoss()\n",
        "        self.accuracy = Accuracy()\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        return\n",
        "\n",
        "    def log_params(self):\n",
        "        return\n",
        "\n",
        "    def load_model(self, restore_path=''):\n",
        "        if restore_path == '':\n",
        "            self.step = 0\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        return\n",
        "\n",
        "    def run_train(self):\n",
        "        print('Run train ...')\n",
        "\n",
        "        self.load_model()\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            self.network.train()\n",
        "\n",
        "            for features, label in self.train_loader:\n",
        "                # Reset gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                output = self.network(features)\n",
        "                \n",
        "                # Calculate error and backpropagate\n",
        "                loss = self.loss(output, torch.tensor(label, dtype=torch.float32))\n",
        "\n",
        "                output = torch.sigmoid(output)\n",
        "\n",
        "                loss.backward()\n",
        "                acc = self.accuracy(output, label).item()\n",
        "\n",
        "                # Update weights with gradients\n",
        "                self.optimizer.step()\n",
        "\n",
        "                self.train_writer.add_scalar('CrossEntropyLoss', loss, self.step)\n",
        "                self.train_writer.add_scalar('Accuracy', acc, self.step)\n",
        "\n",
        "                self.step += 1\n",
        "\n",
        "                if self.step % 50 == 0:\n",
        "                    print('EPOCH %d STEP %d : train_loss: %f train_acc: %f' %\n",
        "                          (epoch, self.step, loss.item(), acc))\n",
        "\n",
        "            #self.train_writer.add_histogram('hidden_layer', self.network.linear1.weight.data, self.step)\n",
        "            #self.train_writer.add_histogram('hidden_layer', model.linear1.weight.data, self.step)\n",
        "\n",
        "            # Run validation\n",
        "            running_loss = []\n",
        "            valid_scores = []\n",
        "            valid_labels = []\n",
        "            self.network.eval()\n",
        "            with torch.no_grad():\n",
        "                for features, label in self.val_loader:\n",
        "                    output = self.network(features)\n",
        "                    # Calculate error and backpropagate\n",
        "                    loss = self.loss(output, torch.tensor(label, dtype=torch.float32))\n",
        "\n",
        "                    running_loss.append(loss.item())\n",
        "                    output = torch.sigmoid(output)\n",
        "                    valid_scores.extend(output)\n",
        "                    \n",
        "                    valid_labels.extend(label)\n",
        "\n",
        "            valid_accuracy = self.accuracy(torch.tensor(valid_scores), torch.tensor(valid_labels)).item()\n",
        "            self.valid_writer.add_scalar('CrossEntropyLoss', np.mean(running_loss), self.step)\n",
        "            self.valid_writer.add_scalar('Accuracy', valid_accuracy, self.step)\n",
        "            \n",
        "            print('EPOCH %d : valid_loss: %f valid_acc: %f' % (epoch, np.mean(running_loss), valid_accuracy))\n",
        "        return"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTcTmjAz9Iet"
      },
      "source": [
        "rm -rf /content/tboard_logs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77tUvsIMNrTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f63c9ac-a2ee-4685-98cd-e31d41ac949c"
      },
      "source": [
        "deep_fm = DeepFM()\n",
        "deep_fm.run_train()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run train ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/SiriusDL/week08/deepFM/network.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  first_order_embd_output = self.first_order_embd[col](torch.tensor(input_data[self.emb_columns[i]], dtype=torch.int64))\n",
            "/content/SiriusDL/week08/deepFM/network.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  [first_order_embd_output, self.first_order_embd[col](torch.tensor(input_data[self.emb_columns[i]], dtype=torch.int64))],\n",
            "/content/SiriusDL/week08/deepFM/network.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(input_data[self.emb_columns[i]], dtype=torch.int64))\n",
            "/content/SiriusDL/week08/deepFM/network.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.second_order_embd[col](torch.tensor(input_data[self.emb_columns[i]], dtype=torch.int64))],\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0 STEP 50 : train_loss: 0.486927 train_acc: 0.783784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH 0 : valid_loss: 0.461073 valid_acc: 0.800913\n",
            "EPOCH 1 STEP 100 : train_loss: 0.335166 train_acc: 0.845946\n",
            "EPOCH 1 : valid_loss: 0.352392 valid_acc: 0.842934\n",
            "EPOCH 2 STEP 150 : train_loss: 0.320412 train_acc: 0.851351\n",
            "EPOCH 2 : valid_loss: 0.325733 valid_acc: 0.855052\n",
            "EPOCH 3 STEP 200 : train_loss: 0.332980 train_acc: 0.845946\n",
            "EPOCH 3 : valid_loss: 0.318405 valid_acc: 0.858829\n",
            "EPOCH 4 STEP 250 : train_loss: 0.299547 train_acc: 0.862162\n",
            "EPOCH 4 : valid_loss: 0.314936 valid_acc: 0.859144\n",
            "EPOCH 5 STEP 300 : train_loss: 0.351934 train_acc: 0.829730\n",
            "EPOCH 5 : valid_loss: 0.312697 valid_acc: 0.860403\n",
            "EPOCH 6 STEP 350 : train_loss: 0.311208 train_acc: 0.851351\n",
            "EPOCH 6 : valid_loss: 0.311219 valid_acc: 0.860403\n",
            "EPOCH 7 STEP 400 : train_loss: 0.307016 train_acc: 0.862162\n",
            "EPOCH 7 : valid_loss: 0.310168 valid_acc: 0.864023\n",
            "EPOCH 8 STEP 450 : train_loss: 0.312038 train_acc: 0.867568\n",
            "EPOCH 8 : valid_loss: 0.310095 valid_acc: 0.862921\n",
            "EPOCH 9 STEP 500 : train_loss: 0.279862 train_acc: 0.867568\n",
            "EPOCH 9 : valid_loss: 0.309858 valid_acc: 0.863236\n",
            "EPOCH 10 STEP 550 : train_loss: 0.319114 train_acc: 0.845946\n",
            "EPOCH 10 : valid_loss: 0.309704 valid_acc: 0.863865\n",
            "EPOCH 11 STEP 600 : train_loss: 0.309746 train_acc: 0.845946\n",
            "EPOCH 11 : valid_loss: 0.309518 valid_acc: 0.861662\n",
            "EPOCH 12 STEP 650 : train_loss: 0.349925 train_acc: 0.821622\n",
            "EPOCH 12 : valid_loss: 0.309540 valid_acc: 0.863078\n",
            "EPOCH 13 STEP 700 : train_loss: 0.275840 train_acc: 0.883784\n",
            "EPOCH 13 : valid_loss: 0.309854 valid_acc: 0.863865\n",
            "EPOCH 14 STEP 750 : train_loss: 0.306911 train_acc: 0.843243\n",
            "EPOCH 14 : valid_loss: 0.311263 valid_acc: 0.862291\n",
            "EPOCH 15 STEP 800 : train_loss: 0.261173 train_acc: 0.875676\n",
            "EPOCH 15 : valid_loss: 0.311080 valid_acc: 0.862764\n",
            "EPOCH 16 STEP 850 : train_loss: 0.310272 train_acc: 0.851351\n",
            "EPOCH 16 : valid_loss: 0.309439 valid_acc: 0.862449\n",
            "EPOCH 17 STEP 900 : train_loss: 0.294957 train_acc: 0.872973\n",
            "EPOCH 17 : valid_loss: 0.310789 valid_acc: 0.861032\n",
            "EPOCH 18 STEP 950 : train_loss: 0.378072 train_acc: 0.832432\n",
            "EPOCH 18 : valid_loss: 0.311216 valid_acc: 0.862764\n",
            "EPOCH 19 STEP 1000 : train_loss: 0.245022 train_acc: 0.881081\n",
            "EPOCH 19 : valid_loss: 0.310591 valid_acc: 0.862606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUch6FqNUve2"
      },
      "source": [
        "#%tensorboard --logdir /content/logs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMILqQkqdK4a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}